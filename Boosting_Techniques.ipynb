{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "   -> Boosting is an ensemble learning technique in machine learning where multiple weak learners (usually shallow decision trees) are combined sequentially to form a strong predictive model.\n",
        "   * A weak learner is a model that performs slightly better than random guessing (e.g., a decision stump - a tree with just one split).\n",
        "   * Boosting builds models one after another, where each new model tries to fix the errors made by the previous ones\n",
        "* How Boosting Works\n",
        "   * Start with a weak model (e.g., a shallow decision tree).\n",
        "   * Calculate errors on the training data.\n",
        "   * Give higher weights to misclassified samples (harder points get more attention).\n",
        "   * Train the next weak learner to focus on those misclassified samples.\n",
        "   * Combine all weak learners (weighted vote or sum) to form a strong final model.\n",
        "* This sequential correction process ensures the model improves step by step\n",
        "* How Boosting Improves Weak Learners\n",
        "   * Error focusing: Each learner focuses on the mistakes of its predecessors, reducing bias.\n",
        "   * Weighted combination: Instead of equal votes (like Bagging/Random Forest), Boosting assigns higher importance to stronger learners.\n",
        "   * Bias reduction: By correcting mistakes iteratively, the final model achieves low bias while still controlling variance\n",
        "* Popular Boosting Algorithms\n",
        "  * AdaBoost (Adaptive Boosting) - assigns higher weights to misclassified samples.\n",
        "  * Gradient Boosting - reduces errors by optimizing a loss function step-by-step.\n",
        "  * XGBoost, LightGBM, CatBoost - efficient, scalable implementations widely used in real-world problems\n",
        "\n",
        "Q2.  What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "  \n",
        "   -> 1. AdaBoost (Adaptive Boosting)\n",
        " * Training process:\n",
        "\n",
        "   (1)Start by training a weak learner (e.g., a shallow decision tree).\n",
        "\n",
        "   (2)Compute the errors: misclassified samples get higher weights so that the next learner focuses more on them.\n",
        "   \n",
        "   (3)Each weak learner is assigned a weight depending on its accuracy.\n",
        "   (4)Final prediction is made by a weighted vote (classification) or weighted sum (regression) of all learners.\n",
        "* Key idea: AdaBoost adjusts sample weights — each model tries to correct the mistakes of the previous one by paying more attention to the “hard” cases.\n",
        "\n",
        "* 2.Gradient Boosting :-\n",
        "   * Training process:\n",
        "\n",
        "     1)Start with an initial model (e.g., predicting the mean value for regression).\n",
        "\n",
        "     2)Calculate the residuals (errors) = actual - predicted values.\n",
        "\n",
        "     3)fit the next weak learner to predict these residuals (i.e., to reduce the loss function).\n",
        "\n",
        "     4)Update predictions by adding the new learner’s output (scaled by a learning rate).\n",
        "\n",
        "     5)Repeat iteratively until convergence.\n",
        "  * Key idea: Gradient Boosting optimizes a loss function (e.g., MSE for regression, log-loss for classification) using a gradient descent–like process. Instead of reweighting samples, it fits new learners to residuals (gradients of the loss).\n",
        "\n",
        "\n",
        "Q3. How does regularization help in XGBoost?\n",
        "   -> Regularization in Machine Learning\n",
        "      Regularization is a technique that penalizes model complexity to prevent overfitting.\n",
        "      In XGBoost, it directly controls how trees are built by discouraging overly complex structures.\n",
        "* How Regularization Works in XGBoost:-\n",
        "      * XGBoost's objective function is:\n",
        "\n",
        "                            Obj=Loss+Ω(ft)\n",
        "       * Loss → Measures how well the model fits the data (e.g., squared error, log-loss).\n",
        "       * Ω(ft) → Regularization term that penalizes complex trees.\n",
        "\n",
        "*  Benefits of Regularization in XGBoost\n",
        "\n",
        "     (1) Prevents Overfitting\n",
        "\n",
        "       * By penalizing too many leaves (γ), it avoids overly deep/complex trees.\n",
        "       * By shrinking leaf weights (λ, α), it avoids extreme predictions.\n",
        "\n",
        "     (2) Encourages Simpler Models\n",
        "\n",
        "       * Smaller trees with fewer splits → more generalizable.\n",
        "\n",
        "     (3) Feature Selection Effect (when L1 is used)\n",
        "\n",
        "       * Some leaf weights become zero → less important features are ignored automatically.\n",
        "\n",
        "     (4) Stability & Robustness\n",
        "\n",
        "       * Helps the model perform well on unseen data by balancing fit and complexity.\n",
        "*  Intuition\n",
        "     * Without regularization → the model tries too hard to fit training data (high variance).\n",
        "     * With regularization → the model trades a tiny bit of training accuracy for better generalization (lower test error).\n",
        "\n",
        "\n",
        "Q4. Why is CatBoost considered efficient for handling categorical data?\n",
        "   -> CatBoost is considered highly efficient for handling categorical data because it was specifically designed with techniques that avoid the limitations of traditional encoding methods like one-hot encoding or label encoding.\n",
        " ### Key Reasons CatBoost Handles Categorical Data Efficiently :-\n",
        "  1) No Need for Explicit Encoding (like One-Hot/Label Encoding)\n",
        "     * Most ML algorithms require categorical features to be converted into numeric form (e.g., one-hot or label encoding).\n",
        "     * CatBoost directly accepts categorical features as input and internally transforms them into numerical representations in a more meaningful way.\n",
        "     * This saves time, memory, and preprocessing effort.\n",
        " 2) Efficient Handling with Target Statistics (Ordered Encoding)\n",
        "    * CatBoost uses ordered target statistics to encode categorical variables.\n",
        "    * Instead of simply replacing a category with the mean target value (which risks target leakage), CatBoost uses a permutation-based approach where encoding for a sample is calculated only using data seen before that sample.\n",
        "    * This avoids leakage and overfitting.\n",
        " 3) Efficient with High Cardinality Features\n",
        "    * One-hot encoding blows up feature space when categories are many (e.g., ZIP codes).\n",
        "    * CatBoost avoids this by computing compact numeric encodings, making it efficient even with thousands of categories.\n",
        " 4) Supports Combinations of Categorical Features\n",
        "    * CatBoost automatically generates combinations of categorical features (e.g., City + Product) and encodes them in the same efficient manner.\n",
        "    * This helps capture complex interactions without manual feature engineering.\n",
        " 5) Works Well with Default Settings\n",
        "    * Unlike other algorithms (e.g., XGBoost, LightGBM) that need manual preprocessing or careful parameter tuning for categorical data, CatBoost handles them out of the box, making it user-friendly\n",
        "\n",
        "Q5. What are some real-world applications where boosting techniques are\n",
        "     preferred over bagging methods?   \n",
        "      -> Both bagging (e.g., Random Forests) and boosting (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) are ensemble methods, but boosting is often preferred in applications where accuracy and handling complex patterns are more important than speed or interpretability.\n",
        "### Real-World Applications Where Boosting is Preferred\n",
        "   1. Credit Risk & Loan Default Prediction (Finance)\n",
        "      * Boosting models (especially XGBoost & CatBoost) are widely used in banking and fintech to predict loan defaults, fraud detection, and credit scoring.\n",
        "      * Reason: Boosting can handle imbalanced data (many more non-defaulters than defaulters) and extract subtle patterns that bagging might miss.\n",
        "  2. Customer Churn Prediction (Telecom & SaaS)\n",
        "      * Boosting is ideal for predicting whether a customer will stop    using a service.\n",
        "      * Reason: It captures complex feature interactions (e.g., usage behavior + demographics + complaints) better than bagging.\n",
        "  3. Click-Through Rate (CTR) Prediction in Online Advertising\n",
        "      * Tech giants (Google, Facebook, etc.) use boosting methods like GBDT and CatBoost for predicting ad click probabilities.\n",
        "      * Reason: CTR data is high-dimensional, sparse, and has categorical features → CatBoost especially excels here.\n",
        "  4. Medical Diagnosis & Healthcare Predictions\n",
        "     * Example: Predicting disease likelihood, hospital readmissions, or drug response.\n",
        "     * Reason: Boosting reduces bias and provides higher accuracy in sensitive cases where false negatives/positives are costly.\n",
        "  5. E-commerce: Product Recommendation & Pricing\n",
        "     * Boosting can predict what product a customer is likely to buy or at what price point.\n",
        "     * Reason: It captures nonlinear interactions between browsing history, demographics, and past purchases.\n",
        "  6. Fraud Detection (Banking, Insurance, E-commerce)\n",
        "     * Detecting fraudulent transactions or claims is a classic boosting use case.\n",
        "     * Reason: Boosting models are highly sensitive to outliers and rare patterns, which is crucial for catching fraud.\n",
        "  7. Competitions (e.g., Kaggle, Analytics Vidhya)\n",
        "     * XGBoost, LightGBM, and CatBoost dominate competitions.\n",
        "     *Reason: They consistently outperform bagging methods in terms of accuracy, ROC-AUC, and log-loss, especially on structured/tabular data.     \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y5B4xgYS0QhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. Write a Python program to:\n",
        "#● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "#● Print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV684ufaXlKr",
        "outputId": "98c8e35f-72d4-4c89-837d-81aff2097f63"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Write a Python program to:\n",
        "# ● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "# ● Evaluate performance using R-squared score\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R² Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPS432tPYBPW",
        "outputId": "88ecd9bc-91ff-4d84-a8b2-1d5bc193c31a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R² Score: 0.8004451261281281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. Write a Python program to:\n",
        "# ● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "# ● Tune the learning rate using GridSearchCV\n",
        "# ● Print the best parameters and accuracy\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5V-yRhGZWpG",
        "outputId": "f72aae1b-2a7c-48df-f736-f26af6f1cc6e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Best Cross-Validation Accuracy: 0.9670329670329672\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:43:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9.Write a Python program to:\n",
        "# ● Train a CatBoost Classifier\n",
        "# ● Plot the confusion matrix using seaborn\n",
        "\n",
        "\n",
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrBfw6whgX-G",
        "outputId": "fc927b73-ebf8-45b1-8217-7190bbc3abe3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "model = CatBoostClassifier(iterations=200, learning_rate=0.05, depth=6, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "uHyG7Sjqg7tJ",
        "outputId": "be3ed0de-e0ae-42d5-b7e5-bfec88202ad8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost,XGBoost,or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "  ### ->  1) Problem framing & split\n",
        " * Target: default (1) vs non-default (0). Decide the decision point (e.g., at application time or monthly monitoring) so features don’t peek into the future.\n",
        "  * Split:\n",
        "     * If timestamps exist → time-based split (train on older months,   validate/test on newer months).\n",
        "     * Else → stratified by target; if multiple loans per customer, use GroupKFold by customer_id.\n",
        "  * Leakage control: only use events available before the prediction timestamp; aggregate transactions with strictly left-closed windows.\n",
        "\n",
        " ### 2) Data preprocessing\n",
        "  * 2.1 Feature building (most lift usually comes from here)\n",
        "      * Aggregate transactions into rolling windows (e.g., 30/60/90 days):\n",
        "        * Avg/median balance, volatility (std/CV), minimum balance, overdraft count, days below ₹X, late-payment count, bounce rate.\n",
        "        * Income inflow stability, expense concentration by merchant category, max debit in last 30 days, ratio of essentials vs discretionary spend.\n",
        "      * RFM-style features: Recency of largest inflow/outflow, Frequency of transactions, Monetary totals.\n",
        "      * From demographics/credit: age bands, employment length, debt-to-income, utilization, #enquiries, prior delinquencies.\n",
        "      * Create policy features you may want monotonic with risk (e.g., “# missed payments last 90d” ↑ → risk ↑).\n",
        " *  2.2 Missing values :-\n",
        "    * Tree boosting doesn’t need scaling; handle missing as:\n",
        "       * Add missing-indicator flags for important fields (often predictive).\n",
        "       * XGBoost/CatBoost: can ingest NaN directly; imputation only if an entire feature is mostly missing or for business readability.\n",
        "      * If you must impute: median for numeric; “Unknown” bucket for categoricals; never impute using target.\n",
        " * 2.3 Categorical encoding:-\n",
        "    * Preferred (simple & strong): CatBoost with cat_features → native handling (ordered target statistics), robust to high cardinality.\n",
        "      * If using XGBoost/AdaBoost:\n",
        "      * Low-cardinality → one-hot.\n",
        "      * High-cardinality → K-fold target encoding or hashing trick (avoid leakage by doing it inside CV folds).\n",
        " *  2.4 Class imbalance :-\n",
        "    * Start with cost-sensitive learning rather than resampling:\n",
        "       * XGBoost: set scale_pos_weight = (#neg / #pos) (or tuned).\n",
        "       * CatBoost: use class_weights=[w0, w1] or provide per-row weights.\n",
        "    * Optionally try undersampling majority class on training folds or SMOTE (for tabular may hurt; use carefully and only inside CV).\n",
        "### 3) Model choice: AdaBoost vs XGBoost vs CatBoost\n",
        "     * AdaBoost: simple, fast on clean tabular data; tends to underperform with noise/missing/categoricals and complex interactions.\n",
        "     * XGBoost: strong baseline, handles numeric + NaN, huge ecosystem; requires careful encoding for categoricals and more tuning.\n",
        "     * CatBoost: best default for mixed numeric/categorical, handles NaN, uses ordered boosting to reduce target leakage in encodings, often needs less tuning.\n",
        "* Recommendation here\n",
        "    * If you have many categorical fields (merchant, employer, city, device, …) and non-trivial missingness → start with CatBoost.\n",
        "    * If categories are few and you already have good encoders or latency constraints favor XGB → XGBoost is great.\n",
        "    * Use AdaBoost only as a lightweight benchmark.\n",
        "### 4) Training & hyperparameter tuning\n",
        "Use stratified/time-series CV with early stopping. Prefer RandomizedSearch → Bayesian/Optuna once the search space is scoped.\n",
        "   ##CatBoost (binary:logloss, probability output)\n",
        "  * Start:\n",
        "        * iterations: 200-2000 with early_stopping_rounds=100\n",
        "        * learning_rate: 0.02-0.2\n",
        "        * depth: 4-10\n",
        "        * l2_leaf_reg: 1-10\n",
        "        * bagging_temperature: 0-1\n",
        "        * class_weights: [1, w_pos] (tune w_pos around imbalance ratio)\n",
        "  * Useful options: loss_function='Logloss', eval_metric='AUC' (and monitor PR-AUC offline), monotone_constraints if policy requires.\n",
        "## XGBoost (binary:logistic)\n",
        "   * Start:\n",
        "         * n_estimators: 300-3000 with early stopping\n",
        "         * max_depth: 3-8, min_child_weight: 1-10\n",
        "         * gamma: 0-5\n",
        "         * subsample: 0.6-1.0, colsample_bytree: 0.6-1.0\n",
        "         * learning_rate: 0.01-0.2\n",
        "         * reg_alpha: 0-10, reg_lambda: 1-20\n",
        "         * scale_pos_weight: around neg/pos, then fine-tune\n",
        "    * Consider monotone_constraints on sensitive features.\n",
        "## General tips\n",
        "  * Always keep a calibration step (Platt/Isotonic) after model lock if you’ll use probability thresholds or expected-loss ranking.\n",
        "  * Use feature importance + SHAP to sanity-check drivers and for governance docs.\n",
        "  * Log all configs and seeds; fix a random seed for reproducibility.\n",
        "##5) Evaluation: metrics & thresholding\n",
        "  Because the data is imbalanced and costs are asymmetric:\n",
        "\n",
        "   * Primary ranking metrics: PR-AUC (Average Precision) and ROC-AUC. PR-AUC is more sensitive to class imbalance.\n",
        "   * Operating-point metrics (decisions are thresholded):\n",
        "        * Recall@Precision≥P (e.g., recall when precision ≥ 80%) if you want to keep false positives low.\n",
        "        * Precision@k or Top-k capture rate (what % of all defaulters sit in top k% risk scores) for collections/limit-cut use cases.\n",
        "        * Balanced accuracy, F1/F2 (F2 if missing a defaulter is costlier), MCC as a robust single-number summary.\n",
        "  * Cost/utility: define per-loan costs C_FN (loss when you approve a future defaulter) and C_FP (opportunity loss when you reject a good borrower). Choose the probability threshold τ that maximizes expected profit on the validation set.\n",
        "  * Calibration check: reliability curve/Brier score.\n",
        "## 6) Deployment & monitoring\n",
        "      * Package a feature pipeline (same encoders/aggregations) + model artifact.\n",
        "      * Shadow test before replacing rules; verify latency.\n",
        "      * Monitor: population drift, PSI, calibration drift, stability of top-k capture, subgroup performance (fair lending), and data freshness.\n",
        "      * Retraining cadence: monthly/quarterly or on detected drift.\n",
        "## 7) Example “quick start” choices\n",
        "      * First model: CatBoost with raw NaNs, cat_features list, class_weights=[1, 5–10] (tune), early stopping; PR-AUC as key metric; probability calibration.\n",
        "      * Benchmark: XGBoost with target-encoded high-card categoricals and scale_pos_weight, same CV and metrics.\n",
        "      * Keep AdaBoost as a sanity check.\n",
        "## 8) Business impact\n",
        "      * Lower credit losses: better recall of true defaulters at a chosen precision reduces NPA/write-offs.\n",
        "      * Smarter approvals: approve more good customers at the same risk cap (lift curves show higher approvals for fixed bad-rate).\n",
        "      * Risk-based pricing & limits: use probabilities to set APR/limits; improves unit economics.\n",
        "      * Early-warning & collections: rank delinquency risk to prioritize outreach; reduces roll rates.\n",
        "      * Operational efficiency & governance: explainability (SHAP) supports regulator and auditor reviews; faster underwriting vs manual rules"
      ],
      "metadata": {
        "id": "S1gBajo2hlT0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7c1ad6"
      },
      "source": []
    }
  ]
}